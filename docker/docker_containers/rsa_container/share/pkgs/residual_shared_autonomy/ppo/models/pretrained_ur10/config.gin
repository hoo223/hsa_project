import residual_shared_autonomy.imitation_learning
import residual_shared_autonomy.lunar_lander
import residual_shared_autonomy.ppo

# Parameters for Adam:
# ==============================================================================
Adam.amsgrad = False
Adam.betas = (0.9, 0.999)
Adam.eps = 1e-05
Adam.lr = 0.001
Adam.weight_decay = 0

# Parameters for Checkpointer:
# ==============================================================================
Checkpointer.ckpt_period = 100000
Checkpointer.format = '{:09d}'

# Parameters for ConstrainedResidualPPO:
# ==============================================================================
ConstrainedResidualPPO.base_actor_cls = @UR10RandomActor
ConstrainedResidualPPO.batch_size = 256
ConstrainedResidualPPO.clip_param = 0.2
ConstrainedResidualPPO.ent_coef = 0.01
ConstrainedResidualPPO.env_fn = @make_env
ConstrainedResidualPPO.epochs_per_rollout = 4
ConstrainedResidualPPO.eval_num_episodes = 1
ConstrainedResidualPPO.gamma = 0.99
ConstrainedResidualPPO.gpu = True
ConstrainedResidualPPO.l2_reg = False
ConstrainedResidualPPO.lambda_ = 0.95
ConstrainedResidualPPO.lambda_init = 20
ConstrainedResidualPPO.lambda_lr = 0.003
ConstrainedResidualPPO.lambda_training_start = 20000
ConstrainedResidualPPO.lr_decay_freq = 20000000
ConstrainedResidualPPO.lr_decay_rate = 0.31622776601
ConstrainedResidualPPO.max_grad_norm = 0.5
ConstrainedResidualPPO.nenv = 1
ConstrainedResidualPPO.norm_advantages = True
ConstrainedResidualPPO.optimizer = @optim.Adam
ConstrainedResidualPPO.policy_fn = @ppo_policy_fn
ConstrainedResidualPPO.policy_training_start = 1000
ConstrainedResidualPPO.record_num_episodes = 100
ConstrainedResidualPPO.reward_threshold = -1000
ConstrainedResidualPPO.rollout_length = 512
ConstrainedResidualPPO.vf_coef = 0.5
ConstrainedResidualPPO.wrapper_fn = None

# Parameters for DiagGaussian:
# ==============================================================================
DiagGaussian.constant_log_std = False
DiagGaussian.log_std_max = 2
DiagGaussian.log_std_min = -20

# Parameters for make_env:
# ==============================================================================
make_env.env_id = 'ur10_env:ur10-v0'
make_env.norm_observations = True
make_env.seed = 0

# Parameters for Policy:
# ==============================================================================
# None.

# Parameters for ppo_policy_fn:
# ==============================================================================
ppo_policy_fn.nunits = 128

# Parameters for train:
# ==============================================================================
train.algorithm = @ConstrainedResidualPPO
train.eval = True
train.eval_period = 1000000
train.maxseconds = None
train.maxt = 100000000
train.save_period = 1000000
train.seed = 0

# Parameters for UR10RandomActor:
# ==============================================================================
UR10RandomActor.action_period = 50
UR10RandomActor.space_type = 'task'

# Parameters for VecObsNormWrapper:
# ==============================================================================
VecObsNormWrapper.eps = 0.01
VecObsNormWrapper.log = True
VecObsNormWrapper.log_prob = 0.01
VecObsNormWrapper.mean = None
VecObsNormWrapper.std = None
VecObsNormWrapper.steps = 10000
